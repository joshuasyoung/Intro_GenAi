{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMOsOwuXTqWwA1Ozm0Ndj7I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuasyoung/Intro_GenAi/blob/main/GenAi_Foundations_Ch3_Translation_Seq2Seq_minimal_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packt - Generative AI Foundations in Python - Ch3 - Implementing the original Transfomer (pg 59-68)\n",
        "\n",
        "# English to French translation"
      ],
      "metadata": {
        "id": "WbtvEtN-7ihr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading & preparation"
      ],
      "metadata": {
        "id": "oPu0pZB_9ENo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CYX_eDI67XOR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load demo data:\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Generative-AI-Foundations-in-Python/main/Chapter3/data.csv')\n",
        "\n",
        "# Separate English & French lexicons:\n",
        "EN_TEXT = data.en.to_numpy().tolist()\n",
        "FR_TEXT = data.fr.to_numpy().tolist()\n",
        "\n",
        "# Arbitrarily cap at 100 characters for demonstration to avoid long training times:\n",
        "def demo_limit(vocab, limit=100):\n",
        "  return [i[:limit] for i in vocab]\n",
        "\n",
        "EN_TEXT = demo_limit(EN_TEXT)\n",
        "FR_TEXT = demo_limit(FR_TEXT)\n",
        "\n",
        "# Establish max length of a given sequence:\n",
        "MAX_LEN = 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 rows of the DataFrame to inspect the data\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht5XdIq_9jPc",
        "outputId": "35417ea3-d2d9-4b2e-8568-5d001011e92d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
            "0         14133         58214    15513316   \n",
            "1         12347         67069     9273079   \n",
            "2          7923         27643    17880115   \n",
            "3          3874        105675    18022548   \n",
            "4         17711        105942     9083757   \n",
            "\n",
            "                                                  en  \\\n",
            "0  • United States Medical Staff Honoured for Ext...   \n",
            "1  • Reduce efforts to develop new regulatory gui...   \n",
            "2  The nine First Nations communities participati...   \n",
            "3      Machinery operator for rough mill Production:   \n",
            "4  • June 3, 2000 - Federal-Provincial-Territoria...   \n",
            "\n",
            "                                                  fr  \n",
            "0  • Hommage rendu au personnel médical des États...  \n",
            "1  • réduire les efforts en vue d'élaborer à cour...  \n",
            "2  conseil tribal ont pu acheter des terres et de...  \n",
            "3  Cadres Directeur général, directeur d’usine, d...  \n",
            "4  • Le 3 juin 2000 - Séance de travail fédérale-...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- converting text data into numerical data that can be understood by model"
      ],
      "metadata": {
        "id": "8ceBN7FH_HWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "def train_tokenizer(texts):\n",
        "  tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  trainer = WordPieceTrainer(\n",
        "      vocab_size=5000,\n",
        "      special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"<sos>\", \"<eos>\"],\n",
        "  )\n",
        "  tokenizer.train_from_iterator(texts, trainer)\n",
        "  return tokenizer\n",
        "\n",
        "en_tokenizer = train_tokenizer(EN_TEXT)\n",
        "fr_tokenizer = train_tokenizer(FR_TEXT)"
      ],
      "metadata": {
        "id": "j4MxYKms_DxE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data tensorization\n",
        "- converts numericized text data to tensor format, required for data prep for training."
      ],
      "metadata": {
        "id": "Y1I0V5ivAK-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def tensorize_data(text_data, tokenizer):\n",
        "  numericalized_data = [\n",
        "      torch.tensor(tokenizer.encode(text) .ids) for text in text_data\n",
        "    ]\n",
        "  # Corrected indentation for the next line\n",
        "  padded_data = pad_sequence(numericalized_data, batch_first=True)\n",
        "  # Corrected indentation for the next line\n",
        "  return padded_data\n",
        "\n",
        "src_tensor = tensorize_data(EN_TEXT, en_tokenizer)\n",
        "tgt_tensor = tensorize_data(FR_TEXT, fr_tokenizer)"
      ],
      "metadata": {
        "id": "w90cH8MR_4rx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation\n",
        "- custom dataset created to handle data.  this class is essential for loading data in batches during training"
      ],
      "metadata": {
        "id": "CJG9l_tpBCBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, dataloader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src_data = src_data\n",
        "        self.tgt_data = tgt_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_data[idx], self.tgt_data[idx]\n",
        "\n",
        "dataset = TextDataset(src_tensor, tgt_tensor)"
      ],
      "metadata": {
        "id": "tLaE5arHA3Fz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings Layer\n",
        "- maps each token to continuous vector space.  crucial for model to understand and process data"
      ],
      "metadata": {
        "id": "PTb2mZMkBhcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "      super(Embeddings, self).__init__()\n",
        "      self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.embed(x)"
      ],
      "metadata": {
        "id": "Hcxv4MAABgTl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding\n",
        "- adds position information to embeddings, which helps model understand order of tokens in sequence"
      ],
      "metadata": {
        "id": "4fPBod7LCCGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1,\n",
        "                 max_len=MAX_LEN\n",
        "    ):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1,\n",
        "                 max_len=MAX_LEN\n",
        "    ):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0.0, d_model, 2) * - \\\n",
        "             (math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "fxH_TTBmCBCg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head self-attention (MHSA)\n",
        "- allows model to focus on different parts of input sequence when producing output sequence"
      ],
      "metadata": {
        "id": "Iyi55v3aDOop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COLAB CORRECTION:\n",
        "# Delete the MultiHeadSelfAttention class entirely.\n",
        "# The functionality of self-attention and cross-attention\n",
        "# will be handled directly within EncoderLayer and DecoderLayer\n",
        "# using torch.nn.MultiheadAttention.\n",
        "\n",
        "\n",
        "# ORIGINAL CODE FROM BOOK:\n",
        "# class MultiHeadSelfAttention(nn.Module):\n",
        "#     def __init__(self, d_model, nheads):\n",
        "#         super(MultiHeadSelfAttention, self).__init__()\n",
        "#         self.attention = nn.MultiheadAttention(d_model, nhead)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.attention(x, x, x)"
      ],
      "metadata": {
        "id": "6XHgGwujC_x6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FFN - Fully Connected NN (FCNN)\n",
        "- operates independently on each position"
      ],
      "metadata": {
        "id": "izrlQAK5DqTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))"
      ],
      "metadata": {
        "id": "7I3YIUUpDot1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Layer\n",
        "- consists of MHSA mechanism and simple FFNN.  Structure repeated in stack to form complete encoder"
      ],
      "metadata": {
        "id": "NAJvrf2UEF2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify EncoderLayer to use nn.MultiheadAttention directly\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_ff):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Use nn.MultiheadAttention directly for self-attention\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=0.1, batch_first=False)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure input is in shape (sequence_length, batch_size, d_model) for MultiheadAttention\n",
        "        x = x.transpose(0, 1)\n",
        "        # Self-attention: Query, Key, Value are all the same input tensor\n",
        "        attn_output, _ = self.self_attn(x, x, x)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.norm1(x)\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        # Return to shape (batch_size, sequence_length, d_model)\n",
        "        return self.norm2(x).transpose(0, 1)"
      ],
      "metadata": {
        "id": "FTd_30VEEE4Y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder\n",
        "- stack of identical layers with MHSA mechanism and an FFN"
      ],
      "metadata": {
        "id": "2UcR6NoWGX1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, d_model, nhead, d_ff, num_layers, vocab_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = Embeddings(d_model, vocab_size)\n",
        "    self.pos_encoding = PositionalEncoding(d_model)\n",
        "    # Corrected: Assign ModuleList directly to the instance\n",
        "    self.encoder_layers = nn.ModuleList([\n",
        "        EncoderLayer(d_model, nhead, d_ff) for _ in range(num_layers)\n",
        "    ])\n",
        "    # Removed: FeedForward is part of EncoderLayer, not Encoder\n",
        "    # self.feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.pos_encoding(x)\n",
        "    # Corrected: Iterate through the assigned attribute\n",
        "    for layer in self.encoder_layers:\n",
        "      x = layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "LO_C8y7AGNn0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Layer\n",
        "- similarly, decoders consists of two MHA mechanisms - one self-attention and one cross-attention - followed by an FFN"
      ],
      "metadata": {
        "id": "MknkxAsjHAk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify DecoderLayer to use nn.MultiheadAttention directly\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_ff):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        # Use nn.MultiheadAttention directly for self-attention\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=0.1, batch_first=False)\n",
        "        # Use nn.MultiheadAttention directly for cross-attention\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=0.1, batch_first=False)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        # Ensure inputs are in shape (sequence_length, batch_size, d_model)\n",
        "        x = x.transpose(0, 1)\n",
        "        memory = memory.transpose(0, 1)\n",
        "        # Self-attention in decoder: Query, Key, Value are from the decoder's input\n",
        "        attn_output, _ = self.self_attn(x, x, x)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.norm1(x)\n",
        "        # Cross-attention: Query is from decoder input, Key and Value are from encoder output (memory)\n",
        "        attn_output, _ = self.cross_attn(x, memory, memory)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.norm2(x)\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        # Return to shape (batch_size, sequence_length, d_model)\n",
        "        return self.norm3(x).transpose(0, 1)"
      ],
      "metadata": {
        "id": "ywyal8scG-k1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder\n",
        "- also a stack of identical layers, each containing two MHA mechanisms and an FFN\n",
        "- This stacking layer pattern continues to build transformer architecture.  Each block has a specific role in processing input data and generating output translations"
      ],
      "metadata": {
        "id": "3QnKk_NQQ3k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, nhead, d_ff, num_layers, vocab_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = Embeddings(d_model, vocab_size)\n",
        "    self.pos_encoding = PositionalEncoding(d_model)\n",
        "    self.decoder_layers = nn.ModuleList([\n",
        "        DecoderLayer(d_model, nhead, d_ff) for _ in range(num_layers)\n",
        "    ])\n",
        "    self.linear = nn.Linear(d_model, vocab_size)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "  def forward(self, x, memory):\n",
        "    x = self.embedding(x)\n",
        "    x = self.pos_encoding(x)\n",
        "    for layer in self.decoder_layers:\n",
        "      x = layer(x, memory)\n",
        "    x = self.linear(x)\n",
        "    x = self.softmax(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "xzJB_wFlQ15c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete Transformer\n",
        "- encapsulates previously defined encoder and decoder structures\n",
        "- this is the primary class that will be used for training and translation tasks"
      ],
      "metadata": {
        "id": "kBxB4VhPSCBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Transformer\n",
        "# - encapsulates previously defined encoder and decoder structures\n",
        "# - this is the primary class that will be used for training and translation tasks\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      d_model,\n",
        "      nhead,\n",
        "      d_ff,\n",
        "      num_encoder_layers,\n",
        "      num_decoder_layers,\n",
        "      src_vocab_size,\n",
        "      tgt_vocab_size,\n",
        "  ):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder = Encoder(d_model, nhead, d_ff, \\\n",
        "                           num_encoder_layers, src_vocab_size)\n",
        "    self.decoder = Decoder(d_model, nhead, d_ff, \\\n",
        "                           num_decoder_layers, tgt_vocab_size)\n",
        "\n",
        "  # Define the forward method outside of __init__\n",
        "  def forward(self, src, tgt):\n",
        "    memory = self.encoder(src)\n",
        "    output = self.decoder(tgt, memory)\n",
        "    return output"
      ],
      "metadata": {
        "id": "1FS6etU_Rxi3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training function\n",
        "- iterates through the epochs and batches, calculates the loss and updates model parameters"
      ],
      "metadata": {
        "id": "OXr3Ab74TDe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loss_fn, optimizer, NUM_EPOCHS = 10):\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in batch_iterator:\n",
        "          src, tgt = batch\n",
        "          optimizer.zero_grad()\n",
        "          output = model(src, tgt)\n",
        "          loss = loss_fn(output.view(-1, tgt_vocab_size), tgt.view(-1))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch} Loss {total_loss / len(batch_iterator)}')"
      ],
      "metadata": {
        "id": "oZZsGuhaTCa2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation function\n",
        "- uses the trained model to translate source text into the target language.  \n",
        "- generates a translation token by token and stops when an end-of-sequence (EOS) token is generated or when max length is reached"
      ],
      "metadata": {
        "id": "4sEPluSiUK74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, src_text, src_tokenizer, tgt_tokenizer, max_len=MAX_LEN):\n",
        "  model.eval()\n",
        "  src_tokens = src_tokenizer.encode(src_text).ids\n",
        "  src_tensor = torch.LongTensor(src_tokens).unsqueeze(0)\n",
        "\n",
        "  tgt_sos_idx = tgt_tokenizer.token_to_id(\"<sos>\")\n",
        "  tgt_eos_idx = tgt_tokenizer.token_to_id(\"<eos>\")\n",
        "\n",
        "  tgt_tensor = torch.LongTensor([tgt_sos_idx]).unsqueeze(0)\n",
        "\n",
        "  for i in range(max_len):\n",
        "    with torch.no_grad():\n",
        "      output = model(src_tensor, tgt_tensor)\n",
        "\n",
        "    predicted_token_idx = output.argmax(dim=2)[0,-1].item()\n",
        "def translate(model, src_text, src_tokenizer, tgt_tokenizer, max_len=MAX_LEN):\n",
        "  model.eval()\n",
        "  src_tokens = src_tokenizer.encode(src_text).ids\n",
        "  src_tensor = torch.LongTensor(src_tokens).unsqueeze(0)\n",
        "\n",
        "  tgt_sos_idx = tgt_tokenizer.token_to_id(\"<sos>\")\n",
        "  tgt_eos_idx = tgt_tokenizer.token_to_id(\"<eos>\")\n",
        "\n",
        "  tgt_tensor = torch.LongTensor([tgt_sos_idx]).unsqueeze(0)\n",
        "\n",
        "  for i in range(max_len):\n",
        "    with torch.no_grad():\n",
        "      output = model(src_tensor, tgt_tensor)\n",
        "\n",
        "    predicted_token_idx = output.argmax(dim=2)[0,-1].item()\n",
        "    if predicted_token_idx == tgt_eos_idx:\n",
        "      break\n",
        "    tgt_tensor = torch.cat([tgt_tensor, torch.LongTensor([predicted_token_idx]).unsqueeze(0)], dim=1)\n",
        "\n",
        "  translated_token_ids = tgt_tensor[0, 1:].tolist()\n",
        "  translated_text = tgt_tokenizer.decode(translated_token_ids)\n",
        "\n",
        "  return translated_text\n",
        "  translated_text = tgt_tokenizer.decode(translated_token_ids)\n",
        "\n",
        "  return translated_text"
      ],
      "metadata": {
        "id": "4CqmeL2QUATE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main execution\n",
        "- hyperparameters defined, tokenizer and model are instantiated, and training / translation processes initiated"
      ],
      "metadata": {
        "id": "Y_uxcEy9V24H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader # Make sure DataLoader is imported\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Hyperparameters\n",
        "  num_encoder_layers = 2\n",
        "  num_decoder_layers = 2\n",
        "  dropout_rate = 0.1\n",
        "  embedding_dim = 512\n",
        "  nhead = 8\n",
        "  ffn_hid_dim = 2048\n",
        "  batch_size = 31\n",
        "  learning_rate = 0.001\n",
        "\n",
        "  en_tokenizer = train_tokenizer(EN_TEXT)\n",
        "  fr_tokenizer = train_tokenizer(FR_TEXT)\n",
        "\n",
        "  src_vocab_size = len(en_tokenizer.get_vocab())\n",
        "  tgt_vocab_size = len(fr_tokenizer.get_vocab())\n",
        "\n",
        "  src_tensor = tensorize_data(EN_TEXT, en_tokenizer)\n",
        "  tgt_tensor = tensorize_data(FR_TEXT, fr_tokenizer)\n",
        "\n",
        "  dataset = TextDataset(src_tensor, tgt_tensor)\n",
        "\n",
        "  model = Transformer(\n",
        "      embedding_dim,\n",
        "      nhead,\n",
        "      ffn_hid_dim,\n",
        "      num_encoder_layers,\n",
        "      num_decoder_layers,\n",
        "      src_vocab_size,\n",
        "      tgt_vocab_size,\n",
        "  )\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  batch_iterator = DataLoader(\n",
        "      dataset, batch_size=batch_size,\n",
        "      shuffle=True, drop_last = True)\n",
        "\n",
        "  train(model, loss_fn, optimizer, NUM_EPOCHS= 10)\n",
        "\n",
        "  src_text = \"hello, how are you?\"\n",
        "  translated_text = translate(\n",
        "      model, src_text, en_tokenizer, fr_tokenizer)\n",
        "  print(f\"English: {src_text}\")\n",
        "  print(f\"French: {translated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkVd5v20VqYw",
        "outputId": "b923c17b-91e2-4259-bddf-526f1ce42bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss 7.827403373188442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P40lvVNJX0DI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}